{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy   as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from Data_info import *\n",
    "from math import pi, sqrt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric is Demographic Parity (DP1). Code source https://github.com/steven7woo/fair_regression_reduction/tree/master based on: Fair Regression: Quantitative Definitions and Reduction-based Algorithms https://arxiv.org/abs/1905.12843\n",
    "\n",
    "def get_histogram(pred, theta_indices):\n",
    "    hist, _ = np.histogram(pred, bins=np.append(theta_indices, theta_indices.iloc[-1] + 1))\n",
    "    return pd.Series(hist, index=theta_indices)\n",
    "\n",
    "def calc_p1(pred, sensitive_features):\n",
    "    Theta = np.unique(pred)  \n",
    "    theta_indices = pd.Series(Theta)\n",
    "\n",
    "    histogram_all = get_histogram(pred, theta_indices)\n",
    "    total_count = histogram_all.sum()\n",
    "    PMF_all = histogram_all / total_count\n",
    "\n",
    "    max_DP_disp = 0\n",
    "    for g in np.unique(sensitive_features):\n",
    "        histogram_g = get_histogram(pred[sensitive_features == g], theta_indices)\n",
    "        PMF_g = histogram_g / histogram_g.sum()\n",
    "        max_DP_disp = max(max_DP_disp, np.max(np.abs(np.cumsum(PMF_all) - np.cumsum(PMF_g))))\n",
    "\n",
    "    return max_DP_disp\n",
    "\n",
    "\n",
    "\n",
    "# Metrics are independence and separation. Code source https://dalex.drwhy.ai/python-dalex-fairness-regression.html based on: Fairness Measures for Regression via Probabilistic Classification https://arxiv.org/pdf/2001.06089.pdf\n",
    "\n",
    "def calculate_p4_m1(y, y_hat, protected, privileged):    \n",
    "    unique_protected = np.unique(protected)\n",
    "    unique_unprivileged = unique_protected[unique_protected != privileged]\n",
    "\n",
    "    data = pd.DataFrame(columns=['subgroup', 'independence', 'separation', 'sufficiency'])\n",
    "\n",
    "    for unprivileged in unique_unprivileged:\n",
    "        # filter elements\n",
    "        array_elements = np.isin(protected, [privileged, unprivileged])\n",
    "\n",
    "        y_u = ((y[array_elements] - y[array_elements].mean()) / y[array_elements].std()).reshape(-1, 1)\n",
    "        s_u = ((y_hat[array_elements] - y_hat[array_elements].mean()) / y_hat[array_elements].std()).reshape(-1, 1)\n",
    "\n",
    "        a = np.where(protected[array_elements] == privileged, 1, 0)\n",
    "\n",
    "        p_s = LogisticRegression()\n",
    "        p_ys = LogisticRegression()\n",
    "        p_y = LogisticRegression()\n",
    "    \n",
    "\n",
    "        p_s.fit(s_u, a)\n",
    "        p_y.fit(y_u, a)\n",
    "        p_ys.fit(np.c_[y_u, s_u], a)\n",
    "        pred_p_s = p_s.predict_proba(s_u.reshape(-1, 1))[:, 1]\n",
    "        pred_p_y = p_y.predict_proba(y_u.reshape(-1, 1))[:, 1]\n",
    "        pred_p_ys = p_ys.predict_proba(np.c_[y_u, s_u])[:, 1]\n",
    "\n",
    "        n = len(a)\n",
    "    \n",
    "        r_ind = ((n - a.sum()) / a.sum()) * (pred_p_s / (1 - pred_p_s)).mean()\n",
    "\n",
    "        try:\n",
    "            r_sep = ((pred_p_ys / (1 - pred_p_ys) * (1 - pred_p_y) / pred_p_y)).mean()\n",
    "            r_suf = ((pred_p_ys / (1 - pred_p_ys)) * ((1 - pred_p_s) / pred_p_s)).mean()\n",
    "        except:\n",
    "            print('Error occured')\n",
    "            r_sep = -1\n",
    "            r_suf = -1\n",
    "\n",
    "        to_append = pd.DataFrame({'subgroup': [unprivileged],\n",
    "                                'independence': [r_ind],\n",
    "                                'separation': [r_sep],\n",
    "                                'sufficiency': [r_suf]})\n",
    "\n",
    "        data = pd.concat([data, to_append])\n",
    "\n",
    "    # append the scale\n",
    "    to_append = pd.DataFrame({'subgroup': [privileged],\n",
    "                            'independence': [1],\n",
    "                            'separation': [1],\n",
    "                            'sufficiency': [1]})\n",
    "    ## TODO: this should be uncommented but adds blanks to the plots\n",
    "    # data = pd.concat([data, to_append]) \n",
    "\n",
    "    data.index = data.subgroup\n",
    "    data = data.iloc[:, 1:]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Metric is Demographic Parity with Wasserstein Barycenters  (DP2). Code source https://github.com/lucaoneto/NIPS2020_Fairness based on: Fair Regression with Wasserstein Barycenters https://arxiv.org/pdf/2006.07286\n",
    "\n",
    "def f_err(YT, YF):\n",
    "    err = np.mean((YT - YF) ** 2)\n",
    "    return err\n",
    "\n",
    "\n",
    "def calculate_p2(Y, S):\n",
    "    vv = np.unique(S)\n",
    "    nn = [np.sum(S == v) for v in vv]\n",
    "    Y_subsets = [Y[S == v] for v in vv]\n",
    "    tt = np.linspace(min(Y), max(Y), 1000)\n",
    "    sorted_subsets = [np.sort(subset) for subset in Y_subsets]\n",
    "    \n",
    "    cumulative_counts = [np.searchsorted(subset, tt, side='right') for subset in sorted_subsets]\n",
    "    \n",
    "    cdf_values = [counts / n for counts, n in zip(cumulative_counts, nn)]\n",
    "    differences = np.abs(cdf_values[0] - cdf_values[1])\n",
    "    fai = np.max(differences)\n",
    "    \n",
    "    return fai\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Metric is Demographic Parity with Renyi correlation (DP3). Code source https://github.com/criteo-research/continuous-fairness based on: Fairness-Aware Learning for Continuous Attributes and Treatments https://proceedings.mlr.press/v97/mary19a/mary19a.pdf\n",
    "\n",
    "class kde:\n",
    "    \"\"\"\n",
    "    A Gaussian KDE implemented in pytorch for the gradients to flow in pytorch optimization.\n",
    "\n",
    "    Keep in mind that KDE are not scaling well with the number of dimensions and this implementation is not really\n",
    "    optimized...\n",
    "    \"\"\"\n",
    "    def __init__(self, x_train):\n",
    "        n, d = x_train.shape\n",
    "\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "\n",
    "        self.bandwidth = (n * (d + 2) / 4.) ** (-1. / (d + 4))\n",
    "        self.std = self.bandwidth\n",
    "\n",
    "        self.train_x = x_train\n",
    "\n",
    "    def pdf(self, x):\n",
    "        s = x.shape\n",
    "        d = s[-1]\n",
    "        s = s[:-1]\n",
    "        assert d == self.d\n",
    "\n",
    "        data = x.unsqueeze(-2)\n",
    "\n",
    "        train_x = _unsqueeze_multiple_times(self.train_x, 0, len(s))\n",
    "\n",
    "        pdf_values = (\n",
    "                        torch.exp(-((data - train_x).norm(dim=-1) ** 2 / (self.bandwidth ** 2) / 2))\n",
    "                    ).mean(dim=-1) / sqrt(2 * pi) / self.bandwidth\n",
    "\n",
    "        return pdf_values\n",
    "\n",
    "\n",
    "def _unsqueeze_multiple_times(input, axis, times):\n",
    "    \"\"\"\n",
    "    Utils function to unsqueeze tensor to avoid cumbersome code\n",
    "    :param input: A pytorch Tensor of dimensions (D_1,..., D_k)\n",
    "    :param axis: the axis to unsqueeze repeatedly\n",
    "    :param times: the number of repetitions of the unsqueeze\n",
    "    :return: the unsqueezed tensor. ex: dimensions (D_1,... D_i, 0,0,0, D_{i+1}, ... D_k) for unsqueezing 3x axis i.\n",
    "    \"\"\"\n",
    "    output = input\n",
    "    for i in range(times):\n",
    "        output = output.unsqueeze(axis)\n",
    "    return output\n",
    "\n",
    "# Independence of 2 variables\n",
    "def _joint_2(X, Y, density, damping=1e-10):\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    Y = (Y - Y.mean()) / Y.std()\n",
    "    data = torch.cat([X.unsqueeze(-1), Y.unsqueeze(-1)], -1)\n",
    "    joint_density = density(data)\n",
    "\n",
    "    nbins = int(min(50, 5. / joint_density.std))\n",
    "    #nbins = np.sqrt( Y.size/5 )\n",
    "    x_centers = torch.linspace(-2.5, 2.5, nbins)\n",
    "    y_centers = torch.linspace(-2.5, 2.5, nbins)\n",
    "\n",
    "    xx, yy = torch.meshgrid([x_centers, y_centers])\n",
    "    grid = torch.cat([xx.unsqueeze(-1), yy.unsqueeze(-1)], -1)\n",
    "    h2d = joint_density.pdf(grid) + damping\n",
    "    h2d /= h2d.sum()\n",
    "    return h2d\n",
    "\n",
    "\n",
    "def calculate_p3(X, Y, damping = 1e-10):\n",
    "    \"\"\"\n",
    "    An estimator of the Hirschfeld-Gebelein-Renyi maximum correlation coefficient using Witsenhausenâ€™s Characterization:\n",
    "    HGR(x,y) is the second highest eigenvalue of the joint density on (x,y). We compute here the second eigenvalue on\n",
    "    an empirical and discretized density estimated from the input data.\n",
    "    :param X: A torch 1-D Tensor\n",
    "    :param Y: A torch 1-D Tensor\n",
    "    :param density: so far only kde is supported\n",
    "    :return: numerical value between 0 and 1 (0: independent, 1:linked by a deterministic equation)\n",
    "    \"\"\"\n",
    "    h2d = _joint_2(X, Y, kde, damping=damping)\n",
    "    marginal_x = h2d.sum(dim=1).unsqueeze(1)\n",
    "    marginal_y = h2d.sum(dim=0).unsqueeze(0)\n",
    "    Q = h2d / (torch.sqrt(marginal_x) * torch.sqrt(marginal_y))\n",
    "    return torch.svd(Q)[1][1]\n",
    "\n",
    "\n",
    "\n",
    "def _joint_3(X, Y, Z, density, damping=1e-10):\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    Y = (Y - Y.mean()) / Y.std()\n",
    "    Z = (Z - Z.mean()) / Z.std()\n",
    "    data = torch.cat([X.unsqueeze(-1), Y.unsqueeze(-1), Z.unsqueeze(-1)], -1)\n",
    "    joint_density = density(data)  # + damping\n",
    "\n",
    "    nbins = int(min(50, 5. / joint_density.std))\n",
    "    x_centers = torch.linspace(-2.5, 2.5, nbins)\n",
    "    y_centers = torch.linspace(-2.5, 2.5, nbins)\n",
    "    z_centers = torch.linspace(-2.5, 2.5, nbins)\n",
    "    xx, yy, zz = torch.meshgrid([x_centers, y_centers, z_centers])\n",
    "    grid = torch.cat([xx.unsqueeze(-1), yy.unsqueeze(-1), zz.unsqueeze(-1)], -1)\n",
    "\n",
    "    h3d = joint_density.pdf(grid) + damping\n",
    "    h3d /= h3d.sum()\n",
    "    return h3d\n",
    "\n",
    "\n",
    "def calculate_m2(X, Y, Z):\n",
    "    \"\"\"\n",
    "    An estimator of the function z -> HGR(x|z, y|z) where HGR is the Hirschfeld-Gebelein-Renyi maximum correlation\n",
    "    coefficient computed using Witsenhausenâ€™s Characterization: HGR(x,y) is the second highest eigenvalue of the joint\n",
    "    density on (x,y). We compute here the second eigenvalue on\n",
    "    an empirical and discretized density estimated from the input data.\n",
    "    :param X: A torch 1-D Tensor\n",
    "    :param Y: A torch 1-D Tensor\n",
    "    :param Z: A torch 1-D Tensor\n",
    "    :param density: so far only kde is supported\n",
    "    :return: A torch 1-D Tensor of same size as Z. (0: independent, 1:linked by a deterministic equation)\n",
    "    \"\"\"\n",
    "    damping = 1e-10\n",
    "    h3d = _joint_3(X, Y, Z, kde, damping=damping)\n",
    "    marginal_xz = h3d.sum(dim=1).unsqueeze(1)\n",
    "    marginal_yz = h3d.sum(dim=0).unsqueeze(0)\n",
    "    Q = h3d / (torch.sqrt(marginal_xz) * torch.sqrt(marginal_yz))\n",
    "    return np.array(([torch.svd(Q[:, :, i])[1][1] for i in range(Q.shape[2])]))\n",
    "\n",
    "\n",
    "\n",
    "def calc_all_metrics(pred,sens):\n",
    "    \n",
    "    if (sens == data_class.protected_Hispanic_vs_other).all() :\n",
    "        sens = np.where(sens == 'Hispanic', 0, 1)\n",
    "        group = 'Hispanic_vs_other'\n",
    "    elif (sens == data_class.protected_white_none_Hispanic_vs_other).all() :\n",
    "        sens = np.where(sens == 'Other', 0, 1)\n",
    "        group = 'white_none_Hispanic_vs_other'\n",
    "    elif (sens == data_class.protected_white_vs_other).all() :\n",
    "        sens = np.where(sens == 'None White', 0, 1)\n",
    "        group = 'white_vs_other'\n",
    "    else:\n",
    "        print('Sensitive  attribute not found')\n",
    "        return \n",
    "\n",
    "\n",
    "    uniqe_val = len(set(pred))\n",
    "    mse = data_class.calc_mse_percent_val(pred)\n",
    "    pred = data_class.get_pred_descalled_from_perecent(pred)\n",
    "    err_abs = abs(pred - data_class.y_percentage_val)\n",
    "    \n",
    "    print('group =', group)\n",
    "    print('RMSE =', mse)\n",
    "\n",
    "\n",
    "    p1 = calc_p1(pred,sens)\n",
    "    print('P1 =',p1)\n",
    "\n",
    "    p2 = calculate_p2(pred,sens)\n",
    "    print('P2 =',p2)\n",
    "\n",
    "    p3 = float(calculate_p3((torch.Tensor(pred)),torch.Tensor(sens)))\n",
    "    print('P3 =',p3)\n",
    "\n",
    "    pm = calculate_p4_m1(data_class.y_percentage_val, pred, sens, 1)\n",
    "    \n",
    "    p4 = round(pm['independence'].values[0],2)\n",
    "    m1 = round(pm['separation'].values[0],2)\n",
    "    print('P4 =',p4)\n",
    "    print('M1 =',m1)\n",
    "\n",
    "    m2 =  np.max(calculate_m2(torch.Tensor(pred),torch.Tensor(sens),torch.Tensor(data_class.y_percentage_val)))\n",
    "    print('M2 =',m2)\n",
    "\n",
    "\n",
    "\n",
    "    err_abs_p1 = calc_p1(err_abs,sens)\n",
    "    print('err_abs P1 =',err_abs_p1)\n",
    "\n",
    "    err_abs_p2 = calculate_p2(err_abs,sens)\n",
    "    print('err_abs P2 =',err_abs_p2)\n",
    "\n",
    "    err_abs_p3 = float(calculate_p3((torch.Tensor(err_abs)),torch.Tensor(sens)))\n",
    "    print('err_abs P3 =',err_abs_p3)\n",
    "\n",
    "    pm = calculate_p4_m1(data_class.y_percentage_val, err_abs, sens, 1)\n",
    "    err_abs_p4 = round(pm['independence'].values[0],2)\n",
    "    print('err_abs P4 =',err_abs_p4)\n",
    "\n",
    "\n",
    "    print()\n",
    "\n",
    "    return [group, mse, p1, p2, p3, p4, m1, m2,err_abs_p1 ,err_abs_p2 , err_abs_p3, err_abs_p4,uniqe_val ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_orginal = pd.read_csv(\"../Ml_ready_all_percent.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class = Data_info(data_df_orginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_results = []\n",
    "sens_groups = [data_class.protected_white_vs_other, data_class.protected_Hispanic_vs_other, data_class.protected_white_none_Hispanic_vs_other]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_mse = math.inf\n",
    "max_depth = [4, 6, 12]\n",
    "n_estimators = [25, 50]\n",
    "learning_rate = [0.1, 0.01]\n",
    "\n",
    "\n",
    "for max_d in max_depth:\n",
    "    for n_estim in n_estimators:\n",
    "        for lr in learning_rate:\n",
    "            xgb_model_orginal = xgb.XGBRegressor(n_estimators=n_estim, max_depth=max_d, eta=lr)\n",
    "            xgb_model_orginal.fit(data_class.X_train_scaled, data_class.y_percentage_train_scaled)\n",
    "            y_pred = xgb_model_orginal.predict(data_class.X_test_scaled)\n",
    "            mse = data_class.calc_mse_percent_test(y_pred)\n",
    "            if mse < lowest_mse:\n",
    "                pickle.dump(xgb_model_orginal, open('Orginal_experment_models_and_reults/xgb_model','wb'))\n",
    "                lowest_mse = mse\n",
    "                xgb_best_depth, xgb_best_n, xgb_best_lr = max_d, n_estim, lr\n",
    "                print(\"Best mse=\",lowest_mse, \"The best hyperparameters are: \",'best_depth=',xgb_best_depth, 'best_n=',xgb_best_n, 'best_lr=',xgb_best_lr)  \n",
    "            else:\n",
    "                print(\"Skipped\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_orginal = pickle.load(open('Orginal_experment_models_and_reults/xgb_model','rb'))\n",
    "y_pred = xgb_model_orginal.predict(data_class.X_val_scaled)\n",
    "np.savetxt(\"Orginal_experment_models_and_reults/xgb_model.csv\", y_pred, delimiter=',')\n",
    "\n",
    "for group in sens_groups:\n",
    "    results = calc_all_metrics(y_pred, group)\n",
    "    results.append('xgb')\n",
    "    fairness_results.append(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_mse = math.inf\n",
    "\n",
    "max_depth = [4, 6, 12]\n",
    "n_estimators = [50, 100, 200]\n",
    "\n",
    "for max_d in max_depth:\n",
    "    for n_estim in n_estimators:\n",
    "        RF_model_orginal = RandomForestRegressor(max_depth=max_d, n_estimators=n_estim, n_jobs= mp.cpu_count())\n",
    "        RF_model_orginal.fit(data_class.X_train_scaled, data_class.y_percentage_train_scaled)\n",
    "        y_pred = RF_model_orginal.predict(data_class.X_test_scaled)\n",
    "        mse = data_class.calc_mse_percent_test(y_pred)\n",
    "        if mse < lowest_mse:\n",
    "            pickle.dump(RF_model_orginal, open('Orginal_experment_models_and_reults/RF_model', 'wb'))\n",
    "            lowest_mse = mse\n",
    "            RF_best_depth,  RF_best_n = max_d, n_estim\n",
    "            print(\"Best mse=\",lowest_mse, \"The best hyperparameters are: \",'best_depth=',RF_best_depth, 'best_n=',RF_best_n)  \n",
    "        else:\n",
    "            print(\"Skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model_orginal = pickle.load(open('Orginal_experment_models_and_reults/RF_model','rb'))\n",
    "y_pred = RF_model_orginal.predict(data_class.X_val_scaled)\n",
    "\n",
    "np.savetxt(\"Orginal_experment_models_and_reults/RF_model.csv\", y_pred, delimiter=',')\n",
    "\n",
    "for group in sens_groups:\n",
    "    results = calc_all_metrics(y_pred, group)\n",
    "    results.append('RF')\n",
    "    fairness_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_mse = math.inf\n",
    "\n",
    "max_width = [8, 16]\n",
    "n_steps = [3, 6]\n",
    "\n",
    "for max_w in max_width:\n",
    "    for n_s in n_steps :\n",
    "        tbn_model_orginal = TabNetRegressor( n_d=max_w, n_a=max_w, n_steps=n_s, device_name=device, verbose=1)\n",
    "        tbn_model_orginal.fit(data_class.X_train_scaled,data_class.y_percentage_train_scaled.reshape(-1,1), patience=10,eval_set=[(data_class.X_test_scaled, data_class.y_percentage_test_scaled.reshape(-1,1))])\n",
    "        y_pred = tbn_model_orginal.predict(data_class.X_test_scaled)\n",
    "        mse = data_class.calc_mse_percent_test(y_pred)\n",
    "        if mse < lowest_mse:\n",
    "            pickle.dump(tbn_model_orginal, open('Orginal_experment_models_and_reults/tbn_model', 'wb'))\n",
    "            lowest_mse = mse\n",
    "            TBN_best_width, TBN_best_n = max_w, n_s, \n",
    "            print(\"Best mse=\",lowest_mse, \"The best hyperparameters are: \",'best_width=',TBN_best_width, 'best_n=',TBN_best_n)  \n",
    "        else:\n",
    "            print(\"Skipped\")    \n",
    "\n",
    "# Best mse= 0.16364967155661278 The best hyperparameters are:  best_width= 16 best_n= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbn_model_orginal = pickle.load(open('Orginal_experment_models_and_reults/tbn_model','rb'))\n",
    "y_pred = tbn_model_orginal.predict(data_class.X_val_scaled)\n",
    "\n",
    "np.savetxt(\"Orginal_experment_models_and_reults/TBN_model.csv\", y_pred, delimiter=',')\n",
    "\n",
    "for group in sens_groups:\n",
    "    results = calc_all_metrics(y_pred, group)\n",
    "    results.append('TBN')\n",
    "    fairness_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_mse = math.inf\n",
    "\n",
    "alpha= [0.001, 0.01, 0.1]\n",
    "hidden_layer_sizes=[(256, ), (256, 128, ), (256, 128, 64, ), (256, 128, 64, 32 )]\n",
    "\n",
    "for hidden in hidden_layer_sizes:\n",
    "    for al in alpha :\n",
    "        mlp_model_orginal = MLPRegressor( hidden_layer_sizes=hidden,verbose=False, alpha=al, early_stopping=True)\n",
    "        mlp_model_orginal.fit(data_class.X_train_scaled, data_class.y_percentage_train_scaled)\n",
    "        y_pred = mlp_model_orginal.predict(data_class.X_test_scaled)\n",
    "        mse = data_class.calc_mse_percent_test(y_pred)\n",
    "        if mse < lowest_mse:\n",
    "            pickle.dump(mlp_model_orginal, open('Orginal_experment_models_and_reults/mlp_model', 'wb'))\n",
    "            lowest_mse = mse\n",
    "            mlp_best_hidden, mlp_best_al = hidden, al, \n",
    "            print(\"Best mse=\",lowest_mse, \"The best hyperparameters are: \",'best_hidden=',mlp_best_hidden, 'best_al=',mlp_best_al)  \n",
    "        else:\n",
    "            print(\"Skipped\")    \n",
    "\n",
    "# Best mse= 0.17042182865687036 The best hyperparameters are:  best_hidden= (256, 128, 64) best_al= 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model_orginal = pickle.load(open('Orginal_experment_models_and_reults/mlp_model','rb'))\n",
    "y_pred = mlp_model_orginal.predict(data_class.X_val_scaled)\n",
    "\n",
    "np.savetxt(\"Orginal_experment_models_and_reults/MLP_model.csv\", y_pred, delimiter=',')\n",
    "\n",
    "for group in sens_groups:\n",
    "    results = calc_all_metrics(y_pred, group)\n",
    "    results.append('MLP')\n",
    "    fairness_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data_class.X_train_scaled, columns=data_class.X_train.columns)\n",
    "\n",
    "new_data = []\n",
    "for ind in range(len(data)):\n",
    "    for i in range(2010,2019):\n",
    "        new_data.append(pd.concat((data.iloc[ind][['beds_redfin','baths_redfin','sqft_redfin','appraised_val_2020','house_age_train']],data.iloc[ind, data.columns.str.contains(f'{i}_', case=False)])))\n",
    "new_data_values = []\n",
    "for i in range(len(new_data)):\n",
    "    new_data_values.append(new_data[i].values)\n",
    "\n",
    "X_train_reshaped_LSTM = []\n",
    "for i in range(0,len(new_data_values),9):\n",
    "    X_train_reshaped_LSTM.append(new_data_values[i:i+9])\n",
    "\n",
    "\n",
    "data = pd.DataFrame(data_class.X_test_scaled, columns=data_class.X_test.columns)\n",
    "\n",
    "new_data = []\n",
    "for ind in range(len(data)):\n",
    "    for i in range(2011,2020):\n",
    "        new_data.append(pd.concat((data.iloc[ind][['beds_redfin','baths_redfin','sqft_redfin','appraised_val_2021','house_age_test']],data.iloc[ind, data.columns.str.contains(f'{i}_', case=False)])))\n",
    "\n",
    "new_data_values = []\n",
    "for i in range(len(new_data)):\n",
    "    new_data_values.append(new_data[i].values)\n",
    "\n",
    "X_test_reshaped_LSTM = []\n",
    "for i in range(0,len(new_data_values),9):\n",
    "    X_test_reshaped_LSTM.append(new_data_values[i:i+9])\n",
    "\n",
    "\n",
    "\n",
    "data = pd.DataFrame(data_class.X_val_scaled, columns=data_class.X_val.columns)\n",
    "\n",
    "new_data = []\n",
    "\n",
    "for ind in range(len(data)):\n",
    "    for i in range(2012,2021):\n",
    "        new_data.append(pd.concat((data.iloc[ind][['beds_redfin','baths_redfin','sqft_redfin','appraised_val_2022','house_age_val']],data.iloc[ind, data.columns.str.contains(f'{i}_', case=False)])))\n",
    "\n",
    "new_data_values = []\n",
    "for i in range(len(new_data)):\n",
    "    new_data_values.append(new_data[i].values)\n",
    "\n",
    "X_val_reshaped_LSTM = []\n",
    "for i in range(0,len(new_data_values),9):\n",
    "    X_val_reshaped_LSTM.append(new_data_values[i:i+9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train_reshaped_LSTM.npy', X_train_reshaped_LSTM)\n",
    "np.save('X_test_reshaped_LSTM.npy', X_test_reshaped_LSTM)\n",
    "np.save('X_val_reshaped_LSTM.npy', X_val_reshaped_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped_LSTM = np.load('Data_reshaped/X_train_reshaped_LSTM.npy')\n",
    "X_test_reshaped_LSTM = np.load('Data_reshaped/X_test_reshaped_LSTM.npy')\n",
    "X_val_reshaped_LSTM = np.load('Data_reshaped/X_val_reshaped_LSTM.npy')\n",
    "\n",
    "X_train = np.array(X_train_reshaped_LSTM)\n",
    "y_train = data_class.y_percentage_train_scaled\n",
    "\n",
    "X_test = np.array(X_test_reshaped_LSTM)\n",
    "y_test = data_class.y_percentage_test_scaled\n",
    "\n",
    "X_val = np.array(X_val_reshaped_LSTM)\n",
    "y_val = data_class.y_percentage_val_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 2, output_size=1, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=25, learning_rate=0.00001, batch_size=64,  device='cpu'):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "        dataset = TensorDataset(torch.from_numpy(X_train).float().to(device), torch.from_numpy(y_train).float().to(device))\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            for inputs, labels in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X_test, device='cpu'):\n",
    "        self.to(device)\n",
    "        dataset = TensorDataset(torch.from_numpy(X_test).float().to(device))\n",
    "        dataloader = DataLoader(dataset, batch_size=64) \n",
    "        predictions = []\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in dataloader:\n",
    "                outputs = self(inputs[0])\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[2]\n",
    "LSTMPredictor = LSTM(input_size)\n",
    "LSTMPredictor.fit(X_train, y_train,device='cuda')\n",
    "\n",
    "y_pred = LSTMPredictor.predict(X_val,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class.calc_mse_percent_val(y_pred.flatten())\n",
    "np.savetxt(\"Orginal_experment_models_and_reults/lstm_model.csv\", y_pred, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_all_metrics(y_pred, sens_groups[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in sens_groups:\n",
    "    results = calc_all_metrics(y_pred, group)\n",
    "    results.append('LSTM')\n",
    "    fairness_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=2, output_size=1, num_layers=1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        gru_out, _ = self.gru(x, h0)\n",
    "        output = self.fc(gru_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=25, learning_rate=0.00001, batch_size=64, device='cpu'):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, labels in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "\n",
    "    \n",
    "    def predict(self, X_test, device='cpu'):\n",
    "        self.to(device)\n",
    "        dataset = TensorDataset(torch.from_numpy(X_test).float().to(device))\n",
    "        dataloader = DataLoader(dataset, batch_size=64)\n",
    "        predictions = []\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in dataloader:\n",
    "                outputs = self(inputs[0])\n",
    "                predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[2]\n",
    "GRUPredictor = GRU(input_size)\n",
    "GRUPredictor.fit(X_train, y_train)\n",
    "y_pred = GRUPredictor.predict(X_val)\n",
    "\n",
    "\n",
    "np.savetxt(\"Orginal_experment_models_and_reults/gru_model.csv\", y_pred, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in sens_groups:\n",
    "    results = calc_all_metrics(y_pred, group)\n",
    "    results.append('GRU')\n",
    "    fairness_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['xgb','RF','TBN','MLP','lstm','gru']\n",
    "fairness_results = []\n",
    "for model in models:\n",
    "    y_pred = np.loadtxt(f\"Orginal_experment_models_and_reults/{model}_model.csv\", delimiter=',')\n",
    "    for group in sens_groups:\n",
    "        results = calc_all_metrics(y_pred.flatten(), group)\n",
    "        results.append(model)\n",
    "        fairness_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Group','RMSE','P1','P2','P3','P4','M1','M2','err_abs_p1' ,'err_abs_p2' , 'err_abs_p3', 'err_abs_p4','Unique values', 'Model']\n",
    "metrics_orginal = pd.DataFrame(fairness_results,columns=cols)\n",
    "metrics_orginal[[ 'Model', 'Group','RMSE','P1','P2','P3','P4','M1','M2','err_abs_p1' ,'err_abs_p2' , 'err_abs_p3', 'err_abs_p4','Unique values']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_orginal[[ 'Model', 'Group','RMSE','P1','P2','P3','P4','M1','M2','err_abs_p1' ,'err_abs_p2' , 'err_abs_p3', 'err_abs_p4','Unique values']].to_csv('metrics_orginal.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [[data_class.protected_white_vs_other,'none_white'],[ data_class.protected_Hispanic_vs_other,'Hispanic'],[ data_class.protected_white_none_Hispanic_vs_other, 'Other']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['xgb','rf','tbn','mlp','lstm','gru']\n",
    "fairness_results2 = []\n",
    "for model in models:    \n",
    "    for group in groups:\n",
    "        y_pred = np.loadtxt(f\"Correlation_removed_models_and_results/{model}_model_cr_results_{group[1]}.csv\", delimiter=',')\n",
    "        results = calc_all_metrics(y_pred, group[0])\n",
    "        results.append(model)\n",
    "        fairness_results2.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Group','RMSE','P1','P2','P3','P4','M1','M2','err_abs_p1' ,'err_abs_p2' , 'err_abs_p3', 'err_abs_p4','Unique values', 'Model']\n",
    "metrics_cr = pd.DataFrame(fairness_results2,columns=cols)\n",
    "metrics_cr[[ 'Model', 'Group','RMSE','P1','P2','P3','P4','M1','M2','err_abs_p1' ,'err_abs_p2' , 'err_abs_p3', 'err_abs_p4','Unique values']] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "housing_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
