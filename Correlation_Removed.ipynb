{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy   as np\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import  mean_squared_error\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from Data_info import *\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_orginal = pd.read_csv(\"../Ml_ready_all_percent.csv\")\n",
    "data_df_orginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class = Data_info(data_df_orginal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = pd.DataFrame(data_class.X_train_scaled,columns= data_class.X_train.columns)\n",
    "X_train_new['Race'] = data_class.protected_white_vs_other\n",
    "X_train_new = pd.get_dummies(X_train_new)\n",
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix on the orginal data \n",
    "\n",
    "correlation_matrix = X_train_new.corr()\n",
    "race_col = 'Race_None White'\n",
    "top_correlations = correlation_matrix[race_col].abs().sort_values(ascending=False).head(20)[1:]\n",
    "print(\"Correlation between 'race' and the top 20 correlating columns:\")\n",
    "print(top_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = CorrelationRemover(sensitive_feature_ids=[race_col])\n",
    "X_cr = cr.fit_transform(X_train_new)\n",
    "X_cr = pd.DataFrame(\n",
    "    X_cr, columns=X_train_new.drop(columns=race_col).columns\n",
    ")\n",
    "X_cr[race_col] = X_train_new[race_col]\n",
    "X_cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X_cr.corr()\n",
    "top_correlations = correlation_matrix[race_col].abs().sort_values(ascending=False).head(25)[1:]\n",
    "print(\"Correlation between rave and the top 10 correlating columns:\")\n",
    "top_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_correlations).head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = pd.DataFrame(data_class.X_test_scaled,columns=data_class.X_test.columns)\n",
    "X_test_new['Race'] = data_class.protected_white_vs_other\n",
    "X_test_new = pd.get_dummies(X_test_new)\n",
    "\n",
    "X_val_new = pd.DataFrame(data_class.X_val_scaled,columns=data_class.X_val.columns)\n",
    "X_val_new['Race'] = data_class.protected_white_vs_other\n",
    "X_val_new = pd.get_dummies(X_val_new)\n",
    "\n",
    "X_test_fitted_none_white = cr.transform(X_test_new)\n",
    "X_val_fitted_none_white = cr.transform(X_val_new)\n",
    "\n",
    "X_train_fitted_none_white = np.array(X_cr.values[:, :-2], dtype=float)\n",
    "X_test_fitted_none_white = np.array(X_test_fitted_none_white[:, :-1], dtype=float)\n",
    "X_val_fitted_none_white = np.array(X_val_fitted_none_white[:, :-1], dtype=float)\n",
    "\n",
    "(X_train_fitted_none_white.shape,X_test_fitted_none_white.shape,X_val_fitted_none_white.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Correlation for the second sensitive groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = pd.DataFrame(data_class.X_train_scaled,columns=data_class.X_train.columns)\n",
    "X_train_new['Race'] = data_class.protected_Hispanic_vs_other\n",
    "X_train_new = pd.get_dummies(X_train_new)\n",
    "\n",
    "correlation_matrix = X_train_new.corr()\n",
    "race_col = 'Race_Hispanic'\n",
    "top_correlations = correlation_matrix[race_col].abs().sort_values(ascending=False).head(20)[1:]\n",
    "print(\"Correlation between 'race' and the top 20 correlating columns:\")\n",
    "top_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_correlations).head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = CorrelationRemover(sensitive_feature_ids=[race_col])\n",
    "X_cr = cr.fit_transform(X_train_new)\n",
    "X_cr = pd.DataFrame(\n",
    "    X_cr, columns=X_train_new.drop(columns=race_col).columns)\n",
    "X_cr[race_col] = X_train_new[race_col]\n",
    "\n",
    "correlation_matrix = X_cr.corr()\n",
    "top_correlations = correlation_matrix[race_col].abs().sort_values(ascending=False).head(25)[1:]\n",
    "\n",
    "print(\"Correlation between rave and the top 10 correlating columns:\")\n",
    "print(top_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_correlations).head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = pd.DataFrame(data_class.X_test_scaled,columns=data_class.X_test.columns)\n",
    "X_test_new['Race'] = data_class.protected_Hispanic_vs_other\n",
    "X_test_new = pd.get_dummies(X_test_new)\n",
    "\n",
    "X_val_new = pd.DataFrame(data_class.X_val_scaled,columns=data_class.X_val.columns)\n",
    "X_val_new['Race'] = data_class.protected_Hispanic_vs_other\n",
    "X_val_new = pd.get_dummies(X_val_new)\n",
    "\n",
    "X_test_fitted_Hispanic = cr.transform(X_test_new)\n",
    "X_val_fitted_Hispanic = cr.transform(X_val_new)\n",
    "\n",
    "X_train_fitted_Hispanic = np.array(X_cr.values[:, :-2], dtype=float)\n",
    "X_test_fitted_Hispanic = np.array(X_test_fitted_Hispanic[:, :-1], dtype=float)\n",
    "X_val_fitted_Hispanic = np.array(X_val_fitted_Hispanic[:, :-1], dtype=float)\n",
    "\n",
    "(X_train_fitted_Hispanic.shape,X_test_fitted_Hispanic.shape,X_val_fitted_Hispanic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Correlation for the third sensitive groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = pd.DataFrame(data_class.X_train_scaled,columns=data_class.X_train.columns)\n",
    "X_train_new['Race'] = data_class.protected_white_none_Hispanic_vs_other\n",
    "X_train_new = pd.get_dummies(X_train_new)\n",
    "\n",
    "# correlation_matrix on the orginal data \n",
    "correlation_matrix = X_train_new.corr()\n",
    "race_col = 'Race_Other'\n",
    "top_correlations = correlation_matrix[race_col].abs().sort_values(ascending=False).head(20)[1:]\n",
    "print(\"Correlation between 'race' and the top 20 correlating columns:\")\n",
    "print(top_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_correlations).head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = CorrelationRemover(sensitive_feature_ids=[race_col])\n",
    "X_cr = cr.fit_transform(X_train_new)\n",
    "X_cr = pd.DataFrame(X_cr, columns=X_train_new.drop(columns=race_col).columns)\n",
    "X_cr[race_col] = X_train_new[race_col]\n",
    "\n",
    "correlation_matrix = X_cr.corr()\n",
    "top_correlations = correlation_matrix[race_col].abs().sort_values(ascending=False).head(25)[1:]\n",
    "\n",
    "print(\"Correlation between rave and the top 10 correlating columns:\")\n",
    "print(top_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_correlations).head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = pd.DataFrame(data_class.X_test_scaled,columns=data_class.X_test.columns)\n",
    "X_test_new['Race'] = data_class.protected_white_none_Hispanic_vs_other\n",
    "X_test_new = pd.get_dummies(X_test_new)\n",
    "\n",
    "X_val_new = pd.DataFrame(data_class.X_val_scaled,columns=data_class.X_val.columns)\n",
    "X_val_new['Race'] = data_class.protected_white_none_Hispanic_vs_other\n",
    "X_val_new = pd.get_dummies(X_val_new)\n",
    "\n",
    "X_test_fitted_Other = cr.transform(X_test_new)\n",
    "X_val_fitted_Other = cr.transform(X_val_new)\n",
    "\n",
    "X_train_fitted_Other = np.array(X_cr.values[:, :-2], dtype=float)\n",
    "X_test_fitted_Other = np.array(X_test_fitted_Other[:, :-1], dtype=float)\n",
    "X_val_fitted_Other = np.array(X_val_fitted_Other[:, :-1], dtype=float)\n",
    "\n",
    "(X_train_fitted_Other.shape,X_test_fitted_Other.shape,X_val_fitted_Other.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data_for_LSTM(train,test,val,group):\n",
    "    data = pd.DataFrame(train, columns=data_class.X_train.columns)\n",
    "\n",
    "    new_data = []\n",
    "    for ind in range(len(data)):\n",
    "        for i in range(2010,2019):\n",
    "            new_data.append(pd.concat((data.iloc[ind][['beds_redfin','baths_redfin','sqft_redfin','appraised_val_2020','house_age_train']],data.iloc[ind, data.columns.str.contains(f'{i}_', case=False)])))\n",
    "    new_data_values = []\n",
    "    for i in range(len(new_data)):\n",
    "        new_data_values.append(new_data[i].values)\n",
    "\n",
    "    X_train_reshaped_LSTM = []\n",
    "    for i in range(0,len(new_data_values),9):\n",
    "        X_train_reshaped_LSTM.append(new_data_values[i:i+9])\n",
    "\n",
    "\n",
    "    #########   \n",
    "    data = pd.DataFrame(test, columns=data_class.X_test.columns)\n",
    "\n",
    "    new_data = []\n",
    "    for ind in range(len(data)):\n",
    "        for i in range(2011,2020):\n",
    "            new_data.append(pd.concat((data.iloc[ind][['beds_redfin','baths_redfin','sqft_redfin','appraised_val_2021','house_age_test']],data.iloc[ind, data.columns.str.contains(f'{i}_', case=False)])))\n",
    "\n",
    "    new_data_values = []\n",
    "    for i in range(len(new_data)):\n",
    "        new_data_values.append(new_data[i].values)\n",
    "\n",
    "    X_test_reshaped_LSTM = []\n",
    "    for i in range(0,len(new_data_values),9):\n",
    "        X_test_reshaped_LSTM.append(new_data_values[i:i+9])\n",
    "\n",
    "\n",
    "\n",
    "    #########   \n",
    "    data = pd.DataFrame(val, columns=data_class.X_val.columns)\n",
    "\n",
    "    new_data = []\n",
    "\n",
    "    for ind in range(len(data)):\n",
    "        for i in range(2012,2021):\n",
    "            new_data.append(pd.concat((data.iloc[ind][['beds_redfin','baths_redfin','sqft_redfin','appraised_val_2022','house_age_val']],data.iloc[ind, data.columns.str.contains(f'{i}_', case=False)])))\n",
    "\n",
    "    new_data_values = []\n",
    "    for i in range(len(new_data)):\n",
    "        new_data_values.append(new_data[i].values)\n",
    "\n",
    "    X_val_reshaped_LSTM = []\n",
    "    for i in range(0,len(new_data_values),9):\n",
    "        X_val_reshaped_LSTM.append(new_data_values[i:i+9])\n",
    "\n",
    "\n",
    "    np.save(f'Data_reshaped/X_train_reshaped_cr_{group}.npy', X_train_reshaped_LSTM)\n",
    "    np.save(f'Data_reshaped/X_test_reshaped_cr_{group}.npy', X_test_reshaped_LSTM)\n",
    "    np.save(f'Data_reshaped/X_val_reshaped_cr_{group}.npy', X_val_reshaped_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reshape_data_for_LSTM(X_train_fitted_Other,X_test_fitted_Other,X_val_fitted_Other,'Other')\n",
    "reshape_data_for_LSTM(X_train_fitted_Hispanic,X_test_fitted_Hispanic,X_val_fitted_Hispanic,'Hispanic')\n",
    "reshape_data_for_LSTM(X_train_fitted_none_white,X_test_fitted_none_white,X_val_fitted_none_white,'none_white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing after removing correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost  - Train on (white vs none white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['Other','Hispanic','none_white']\n",
    "data = [[X_train_fitted_Other,X_test_fitted_Other,X_val_fitted_Other],\n",
    "          [X_train_fitted_Hispanic,X_test_fitted_Hispanic,X_val_fitted_Hispanic],\n",
    "          [X_train_fitted_none_white,X_test_fitted_none_white,X_val_fitted_none_white]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_depth = 12\n",
    "xgb_best_n     = 50\n",
    "xgb_best_lr    = 0.01\n",
    "\n",
    "c = 0\n",
    "for d in data:\n",
    "    train = d[0]\n",
    "    test = d[1]\n",
    "    val =  d[2]\n",
    "    group = groups[c]\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=xgb_best_n, max_depth=xgb_best_depth, eta=xgb_best_lr)\n",
    "    xgb_model.fit(train, data_class.y_percentage_train_scaled)\n",
    "    pickle.dump(xgb_model, open(f'Correlation_removed_models_and_results/xgb_model_cr_{group}','wb'))\n",
    "    y_pred = xgb_model.predict(val)     \n",
    "    np.savetxt(f\"Correlation_removed_models_and_results/xgb_model_cr_results_{group}.csv\", y_pred, delimiter=',')\n",
    "    c+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF  - Train on ( white vs none white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_best_depth = 4\n",
    "RF_best_n     = 200\n",
    "\n",
    "c = 0\n",
    "for d in data:\n",
    "    train = d[0]\n",
    "    test = d[1]\n",
    "    val =  d[2]\n",
    "    group = groups[c]\n",
    "\n",
    "    rf_model = RandomForestRegressor(max_depth=RF_best_depth, n_estimators=RF_best_n, n_jobs= mp.cpu_count())\n",
    "    rf_model.fit(train, data_class.y_percentage_train_scaled)\n",
    "    pickle.dump(rf_model, open(f'Correlation_removed_models_and_results/rf_model_cr_{group}','wb'))\n",
    "    y_pred = rf_model.predict(val)     \n",
    "    np.savetxt(f\"Correlation_removed_models_and_results/rf_model_cr_results_{group}.csv\", y_pred, delimiter=',')\n",
    "    c+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBN  - Train on ( white vs non white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBN_best_width = 8\n",
    "TBN_best_n     = 3\n",
    "\n",
    "c = 0\n",
    "for d in data:\n",
    "    train = d[0]\n",
    "    test = d[1]\n",
    "    val =  d[2]\n",
    "    group = groups[c]\n",
    "\n",
    "    tbn_model = TabNetRegressor(n_d=TBN_best_width, n_a=TBN_best_width, n_steps=TBN_best_n, verbose=0)\n",
    "    tbn_model.fit(train, data_class.y_percentage_train_scaled.reshape(-1,1), patience=10,eval_set=[(test, data_class.y_percentage_test_scaled.reshape(-1,1))])\n",
    "    pickle.dump(tbn_model, open(f'Correlation_removed_models_and_results/tbn_model_cr_{group}','wb'))\n",
    "    y_pred = tbn_model.predict(val)     \n",
    "    np.savetxt(f\"Correlation_removed_models_and_results/tbn_model_cr_results_{group}.csv\", y_pred, delimiter=',')\n",
    "    c+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP  - Train on (white vs none white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_best_hidden= (256) \n",
    "mlp_best_al= 0.001\n",
    "\n",
    "c = 0\n",
    "for d in data:\n",
    "    train = d[0]\n",
    "    test = d[1]\n",
    "    val =  d[2]\n",
    "    group = groups[c]\n",
    "\n",
    "    mlp_model = MLPRegressor( hidden_layer_sizes=mlp_best_hidden,verbose=False, alpha=mlp_best_al, early_stopping=True )\n",
    "    mlp_model.fit(train, data_class.y_percentage_train_scaled)\n",
    "    pickle.dump(mlp_model, open(f'Correlation_removed_models_and_results/mlp_model_cr_{group}','wb'))\n",
    "    y_pred = mlp_model.predict(val)     \n",
    "    np.savetxt(f\"Correlation_removed_models_and_results/mlp_model_cr_results_{group}.csv\", y_pred, delimiter=',')\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "    def fit(self, X_train, y_train,X_val, y_val, epochs=25, learning_rate=0.001, batch_size=32, patience=10):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        best_val_loss = float('inf')\n",
    "        dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs, labels\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_dataloader:\n",
    "                    val_inputs, val_labels = val_inputs, val_labels\n",
    "                    val_outputs = self(val_inputs)\n",
    "                    val_loss += criterion(val_outputs, val_labels).item()\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(f'Early stopping after {patience} epochs without improvement.')\n",
    "                    break\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        inputs = torch.from_numpy(X_test).float()\n",
    "        with torch.no_grad():\n",
    "            predictions = self(inputs)\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "\n",
    "lstm_best_hidden_siz =  2\n",
    "lstm_best_num_layers = 1\n",
    "lstm_best_lr = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM  - Train on (white vs none white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for  group in groups:\n",
    "    train = np.load(f'Data_reshaped/X_train_reshaped_cr_{group}.npy')\n",
    "    test = np.load(f'Data_reshaped/X_test_reshaped_cr_{group}.npy')\n",
    "    val =  np.load(f'Data_reshaped/X_val_reshaped_cr_{group}.npy')\n",
    "    input_size = train.shape[2]\n",
    "\n",
    "\n",
    "    LSTMPredictor = LSTM(input_size, hidden_size=lstm_best_hidden_siz, num_layers=lstm_best_num_layers)\n",
    "    LSTMPredictor.fit(train, data_class.y_percentage_train_scaled, test, data_class.y_percentage_test_scaled)\n",
    "    y_pred = LSTMPredictor.predict(val)     \n",
    "    np.savetxt(f\"Correlation_removed_models_and_results/lstm_model_cr_results_{group}.csv\", y_pred, delimiter=',')\n",
    "    c+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        gru_out, _ = self.gru(x, h0)\n",
    "        output = self.fc(gru_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "    def fit(self, X_train, y_train,  epochs=25, learning_rate=0.01, batch_size=64):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "       \n",
    "        for epoch in range(epochs):\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs, labels\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        inputs = torch.from_numpy(X_test).float()\n",
    "        with torch.no_grad():\n",
    "            predictions = self(inputs)\n",
    "        return predictions.cpu().numpy()\n",
    "    \n",
    "gru_best_hidden_siz =  25\n",
    "gru_best_num_layers = 2\n",
    "gru_best_lr = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for  group in groups:\n",
    "    train = np.load(f'Data_reshaped/X_train_reshaped_cr_{group}.npy')\n",
    "    test = np.load(f'Data_reshaped/X_test_reshaped_cr_{group}.npy')\n",
    "    val =  np.load(f'Data_reshaped/X_val_reshaped_cr_{group}.npy')\n",
    "    input_size = train.shape[2]\n",
    "\n",
    "\n",
    "    GRUPredictor = GRU(input_size, hidden_size=gru_best_hidden_siz, num_layers=gru_best_num_layers)\n",
    "\n",
    "    GRUPredictor.fit(train,  data_class.y_percentage_train_scaled,   learning_rate=gru_best_lr)\n",
    "    y_pred = GRUPredictor.predict(val)     \n",
    "    np.savetxt(f\"Correlation_removed_models_and_results/gru_model_cr_results_{group}.csv\", y_pred, delimiter=',')\n",
    "    c+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "housing_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
